{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9177895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backprop seminar\n",
    "\n",
    "### by Talgat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591fd5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Importance of Understanding Backpropagation\n",
    "\n",
    "Understanding backpropagation is crucial for both researchers and practitioners in the field of deep learning for different reasons:\n",
    "\n",
    "- **For Researchers**: The ability to write custom layers and define their differentiation process is essential.\n",
    "\n",
    "- **For Practitioners**: Questions on backpropagation are very popular on deep learning interviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb599dbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PyTorch is very similar to NumPy, but these are key differences:\n",
    "- **GPU Support** PyTorch tensors can be easily moved to a GPU for accelerated computing. See `device` argument and method `.to(device)`. \n",
    "\n",
    "- **Automatic Differentiation (and Dynamic Computational Graphs)** PyTorch provides a powerful autograd system that automatically computes gradients for tensor operations, making it easy to train neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f86b09",
   "metadata": {},
   "source": [
    "This guide assumes you have a basic understanding of PyTorch. **If you're not familiar**, consider reading the [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) tutorial on the official PyTorch website.\n",
    "\n",
    "I will cover the most important functions for a custom backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradients in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99c08637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "311ae801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9793, -1.0864,  3.1290, -0.3812,  0.8100, -0.7537, -0.6821],\n",
       "        [-0.4295, -0.2838,  0.8261,  0.3421, -1.6663, -4.6581,  0.3815],\n",
       "        [-0.3176, -2.2620,  2.0672, -1.0792, -2.6879, -2.4325, -2.1618],\n",
       "        [-0.4634, -1.0539, -0.7558,  4.3560, -2.1569,  0.8904,  0.9192],\n",
       "        [ 1.2624,  0.2435,  2.3492, -1.5918, -1.8435,  1.1442, -0.4143]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(5, 7, dtype=torch.double, requires_grad=True)\n",
    "norm_value = torch.linalg.norm(A) ** 2\n",
    "norm_value.backward()\n",
    "A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8019095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(A.grad, 2 * A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685ce1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error:\n",
    "# A = torch.randn(5, 7, dtype=torch.double)\n",
    "# norm_value = torch.linalg.norm(A) ** 2\n",
    "# norm_value.backward()\n",
    "# A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad436ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error:\n",
    "# with torch.no_grad():\n",
    "#     A = torch.randn(5, 7, dtype=torch.double, requires_grad=False)\n",
    "#     norm_value = torch.linalg.norm(A) ** 2\n",
    "#     norm_value.backward()\n",
    "#     A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74320289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error:\n",
    "# A = torch.randn(5, 7, dtype=torch.double, requires_grad=False)\n",
    "# norm_value = A ** 2\n",
    "# norm_value.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093b1fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reminder on Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792b938",
   "metadata": {},
   "source": [
    "# Einstein summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0874a2b",
   "metadata": {},
   "source": [
    "`torch.einsum(equation_string, *tensors)`:\n",
    "\n",
    "- `equation_string`: A string that specifies the operation in the form `indices1,...,indices_n->indices_res`\n",
    "\n",
    "- `tensors`: The tensors to apply the operation to.\n",
    "\n",
    "Matrix product:\n",
    "$$\n",
    "(A B)_{ij} = \\sum_{k} a_{ik} b_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "522a25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(3, 5)\n",
    "B = torch.rand(5, 4)\n",
    "matmul = torch.einsum('ik,kj->ij', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a96d3",
   "metadata": {},
   "source": [
    "In NumPy and PyTorch, only a single symbol can be an index. For more complex cases, use [einops library](https://einops.rocks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b14a80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TL;DR Custom Gradients with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b2b11",
   "metadata": {},
   "source": [
    "Compute custom gradients for a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ as follows:\n",
    "\n",
    "1. **Think of a Scalar Function** $g: \\mathbb{R}^m \\to \\mathbb{R}$, that maps $f$'s output to a scalar.\n",
    "\n",
    "2. **Write Chain Rule**: The derivative of $g$ with respect to $x_i$ is:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial x_i} g(f(x)) = \\sum_{k} \\frac{\\partial g(f(x))}{\\partial (f(x)_k)} \\frac{\\partial (f(x)_k)}{\\partial x_i}\n",
    "   $$\n",
    "\n",
    "   - The term $\\frac{\\partial g(f(x))}{\\partial (f(x)_k)}$ is provided (gradient from a subsequent layer). We will call it `grad_output`.\n",
    "   - The second term is the Jacobian of your function. \n",
    "\n",
    "3. **Perform Vector-Jacobian Product** (vJp): Multiply the given gradient vector by $f$'s Jacobian. \n",
    "\n",
    "\n",
    "The result is equal to `einsum('k,ki->i', vector, Jacobian)`, though direct computation of vJp is usually not efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb39740-c6a9-4498-b0f6-e36455280f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matvec by vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2922c0e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suppose $A \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb{R}^{n}$, $f(A, x) = Ax$. \n",
    "\n",
    "And suppose $g$ is some function of $f(A, x)$. It can be multidimensional, but without loss of generality we consider one-dimensional case, e.g. $g(\\cdot) = \\| \\cdot \\|^2$. (Vector and matrix functions can be differenciated in the same way.)\n",
    "\n",
    "Chain-rule for $x$:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial x_{i}} g(f(A, x))\n",
    "    =\n",
    "\\sum\\limits_{k}\n",
    "    \\underbrace{\\dfrac{\\partial g(Ax)}{\\partial (Ax)_k}}_\\text{grad_output[k]}\n",
    "\\cdot\n",
    "    \\underbrace{\\dfrac{\\partial (Ax)_k}{\\partial x_i}}_{\\nabla_x (Ax)[k][i]}\n",
    "    =\n",
    "\\sum\\limits_{k}\n",
    "\\underbrace{\\dfrac{\\partial g(Ax)}{\\partial (Ax)_k}}_\\text{grad_output[k]}\n",
    "    \\cdot\n",
    "a_{ki}\n",
    "    =\n",
    "A^\\top \\text{grad_output}\n",
    "$$ \n",
    "\n",
    "In einsum notation, `einsum('k,ki->i', grad_output, dAx_dx)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52be99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matvec by matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d4fb2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Gradients w.r.t. A:\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial a_{ij}} g(f(A, x))\n",
    "    =\n",
    "\\sum\\limits_{k}\n",
    "    \\underbrace{\\dfrac{\\partial g(Ax)}{\\partial (Ax)_k}}_\\text{$g_k :=$ grad_output[k]}\n",
    "\\cdot\n",
    "    \\underbrace{\\dfrac{\\partial (Ax)_k}{\\partial a_{i, j}}}_{J_{ijk}}\n",
    "$$\n",
    "\n",
    "It is easy to understand that $J_{ijk} = x_j \\cdot [k = i]$, where $[\\cdot]$ is an indicator function. That's why\n",
    "$$\n",
    "\\sum\\limits_{k} g_k f_{i, j, k} = g_i^\\top x_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bca8b",
   "metadata": {},
   "source": [
    "In einsum notation, `einsum('k,ijk->ij', grad_output, dAx_dA)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa425e",
   "metadata": {},
   "source": [
    "## Custom Gradients in PyTorch\n",
    "\n",
    "### Autograd Functions\n",
    "\n",
    "- **Purpose**: Enable fine-grained control over both the forward and backward computations.\n",
    "- **Usage**: Define a class inheriting from `torch.autograd.Function` and implement the `@staticmethod`s `forward` and `backward`.\n",
    "\n",
    "```python\n",
    "from torch.autograd import Function\n",
    "\n",
    "class CustomFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = ...\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9df60-bfe8-4de7-8bb7-a78e7627a56d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Custom gradients in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cc3303e-a758-4d68-80bc-fc9291e97038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class MyMatVec(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, x):\n",
    "        ctx.save_for_backward(A, x)\n",
    "        return torch.mv(A, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        A, x = ctx.saved_tensors  # Retrieve saved tensors\n",
    "        grad_A = grad_output[:, None] @ x[None, :]\n",
    "        grad_x = A.T @ grad_output\n",
    "        return grad_A, grad_x\n",
    "\n",
    "# Alias for applying the operation\n",
    "matvec = MyMatVec.apply\n",
    "\n",
    "# Example usage\n",
    "A = torch.randn(5, 7, dtype=torch.double, requires_grad=True)\n",
    "x = torch.randn(7, dtype=torch.double, requires_grad=True)\n",
    "inputs = (A, x)\n",
    "if gradcheck(matvec, inputs, eps=1e-6, atol=1e-4):\n",
    "    print('OK!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5a7ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backward in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23cb653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0142, -3.2544, -3.9560,  5.5219,  1.1941, -0.7747, -0.4376],\n",
       "        [ 0.8601, -0.0065, -0.4173,  2.2674,  0.7285, -0.0284, -2.1073],\n",
       "        [-1.3998, -0.6446, -1.2715,  0.3235,  1.3581, -3.5613, -1.6263],\n",
       "        [-1.6602, -1.1135,  0.6004,  1.4760,  2.3580, -3.8255, -2.0487],\n",
       "        [-3.1986, -1.6538, -0.4544, -0.5043,  2.9867, -0.9459,  3.6030]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(5, 7, dtype=torch.double, requires_grad=True)\n",
    "norm_value = torch.linalg.norm(A) ** 2\n",
    "norm_value.backward()\n",
    "A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c8f28a-596a-438b-a34b-fd082924826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error:\n",
    "# A = torch.randn(5, 7, dtype=torch.double, requires_grad=False)\n",
    "# norm_value = torch.linalg.norm(A) ** 2\n",
    "# norm_value.backward()\n",
    "# A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5816699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error:\n",
    "# with torch.no_grad():\n",
    "#     A = torch.randn(5, 7, dtype=torch.double, requires_grad=False)\n",
    "#     norm_value = torch.linalg.norm(A) ** 2\n",
    "#     norm_value.backward()\n",
    "#     A.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503f78e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "During training:\n",
    "\n",
    "$\\text{dropout}(x) = \\dfrac{1}{1 - p} \\text{Bernoulli}(1 - p) \\odot x$,\n",
    "\n",
    "where $\\odot$ is an element-wise product, $\\text{Bernoulli}(1 - p)$ is a random binary mask from Bernoulli distribution.\n",
    "\n",
    "During inference do nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88fbf5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class MyDropOutTraining(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, mask, p=0.1):\n",
    "        res = x * mask / (1 - p)\n",
    "        ctx.save_for_backward(mask / (1 - p))\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        scaled_mask, = ctx.saved_tensors\n",
    "        return scaled_mask * grad_output, None, None\n",
    "\n",
    "# Alias for applying the operation\n",
    "dropout = MyDropOutTraining.apply\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(7, dtype=torch.double, requires_grad=True)\n",
    "p = 0.1\n",
    "mask = torch.empty_like(x)\n",
    "mask.bernoulli_(1 - p) \n",
    "inputs = (x, mask, p)\n",
    "# Gradcheck will not work with a mask that is not an input\n",
    "if gradcheck(dropout, inputs, eps=1e-6, atol=1e-4):\n",
    "    print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51855216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SoftMax\n",
    "\n",
    "Softmax converts an arbitrary vector to a vector that sums up to one. It is a typical last layer for multiclass classification problems, where we have to output the probability. \n",
    "\n",
    "$$\n",
    "\\text{softmax}(x)_i = \\frac{\\exp{x_i}}{\\sum_{k} \\exp{x_k}}, \\quad x \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "**Exercise** Implement custom gradients for this function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "792091e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class MySoftMax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = x - x.max()  # for stability\n",
    "        exp_vals = torch.exp(x)\n",
    "        res = exp_vals / torch.sum(exp_vals)\n",
    "        ctx.save_for_backward(x, res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, res = ctx.saved_tensors  \n",
    "        # Your code\n",
    "        return torch.ones_like(x)\n",
    "\n",
    "# Alias for applying the operation\n",
    "softmax = MySoftMax.apply\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(7, dtype=torch.double, requires_grad=True)\n",
    "inputs = x\n",
    "if gradcheck(softmax, inputs, eps=1e-6, atol=1e-4):\n",
    "    print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01fc80d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n",
       "        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n",
       "        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n",
       "        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n",
       "        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.randn(5)\n",
    "\n",
    "torch.einsum('i,j->ij', f, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9467c5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n",
       "        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n",
       "        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n",
       "        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n",
       "        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:, None] @ f[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddacb36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n",
       "        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n",
       "        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n",
       "        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n",
       "        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(-1, 1) @ f.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdea48e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conv1D\n",
    "\n",
    "$g(x, w)[i] = \\sum_{j=0}^{k-1} w[j] \\cdot x[i+j], \\quad x \\in \\mathbb{R}^n, \\ w \\in \\mathbb{R}^k, \\ g(x, w) \\in \\mathbb{R}^{n - k + 1}$\n",
    "\n",
    "Write the backward formula..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6f30f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9784ef5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "! pip install torchdiffeq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4bd26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "# Define the ODE system (function) to be solved\n",
    "class SimpleODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parameters or any other components can be defined here if needed\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # Example ODE: dy/dt = -y\n",
    "        dydt = -y\n",
    "        return dydt\n",
    "\n",
    "# Initial condition\n",
    "y0 = torch.tensor([[1.0]], requires_grad=True)\n",
    "\n",
    "# Time points to solve the ODE at (from t=0 to t=1)\n",
    "t = torch.linspace(0, 1, steps=100)\n",
    "\n",
    "# Instantiate the ODE function\n",
    "func = SimpleODEFunc()\n",
    "\n",
    "# Solve the ODE\n",
    "y = odeint(func, y0, t)\n",
    "\n",
    "y = torch.linalg.norm(y)\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d99697",
   "metadata": {},
   "source": [
    "Deep Equilibrium Models: https://implicit-layers-tutorial.org/deep_equilibrium_models/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
