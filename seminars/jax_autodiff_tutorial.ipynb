{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic differentiation with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main features\n",
    "\n",
    "- Numpy wrapper\n",
    "- Auto-vectorization\n",
    "- Auto-parallelization (SPMD paradigm)\n",
    "- Auto-differentiation\n",
    "- XLA backend and JIT support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to compute gradient of your objective?\n",
    "\n",
    "- Define it as a standard Python function\n",
    "- Call ```jax.grad``` and voila!\n",
    "- Do not forget to wrap these functions with ```jax.jit``` to speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- By default, JAX exploits single-precision numbers ```float32```\n",
    "- You can enable double precision (```float64```) by hands.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x, A, b):\n",
    "    res = A @ x - b\n",
    "    return res @ res\n",
    "\n",
    "gradf = jax.grad(f, argnums=0, has_aux=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random numbers in JAX \n",
    "\n",
    "- JAX focuses on the reproducibility of the runs\n",
    "- Analogue of random seed is **the necessary argument** of all functions that generate something random\n",
    "- More details and references on the design of ```random``` submodule are [here](https://github.com/google/jax/blob/master/design_notes/prng.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "A = jax.random.normal(jax.random.PRNGKey(0), (n, n))\n",
    "b = jax.random.normal(jax.random.PRNGKey(0), (n, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 9.188704584401416e-11\n",
      "Compare speed\n",
      "Analytical gradient\n",
      "1.78 ms ± 26.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Grad function\n",
      "1.05 ms ± 18.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Jitted grad function\n",
      "323 µs ± 5.14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check correctness\", jnp.linalg.norm(gradf(x, A, b) - 2 * A.T @ (A @ x - b)))\n",
    "print(\"Compare speed\")\n",
    "print(\"Analytical gradient\")\n",
    "%timeit 2 * A.T @ (A @ x - b)\n",
    "print(\"Grad function\")\n",
    "%timeit gradf(x, A, b).block_until_ready()\n",
    "jit_gradf = jax.jit(gradf)\n",
    "print(\"Jitted grad function\")\n",
    "%timeit jit_gradf(x, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 0.0\n",
      "Time for hessian\n",
      "21.1 ms ± 449 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Emulate hessian and check correctness 0.0\n",
      "Time of emulating hessian\n",
      "21.1 ms ± 502 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "hess_func = jax.jit(jax.hessian(f))\n",
    "print(\"Check correctness\", jnp.linalg.norm(2 * A.T @ A - hess_func(x, A, b)))\n",
    "print(\"Time for hessian\")\n",
    "%timeit hess_func(x, A, b).block_until_ready()\n",
    "print(\"Emulate hessian and check correctness\", \n",
    "      jnp.linalg.norm(jax.jit(hess_func)(x, A, b) - jax.jacfwd(jax.jacrev(f))(x, A, b)))\n",
    "print(\"Time of emulating hessian\")\n",
    "hess_umul_func = jax.jit(jax.jacfwd(jax.jacrev(f)))\n",
    "%timeit hess_umul_func(x, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward mode vs. backward mode: $m \\ll n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 1.0709183798706824e-10\n",
      "Forward mode\n",
      "14.3 ms ± 248 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Backward mode\n",
      "329 µs ± 5.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "fmode_f = jax.jit(jax.jacfwd(f))\n",
    "bmode_f = jax.jit(jax.jacrev(f))\n",
    "print(\"Check correctness\", jnp.linalg.norm(fmode_f(x, A, b) - bmode_f(x, A, b)))\n",
    "print(\"Forward mode\")\n",
    "%timeit fmode_f(x, A, b).block_until_ready()\n",
    "print(\"Backward mode\")\n",
    "%timeit bmode_f(x, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward mode vs. backward mode: $m \\geq n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def fvec(x, A, b):\n",
    "    y = A @ x + b\n",
    "    return jnp.exp(y - jnp.max(y)) / jnp.sum(jnp.exp(y - jnp.max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grad_fvec = jax.jit(jax.grad(fvec))\n",
    "jac_fvec = jax.jacobian(fvec)\n",
    "fmode_fvec = jax.jit(jax.jacfwd(fvec))\n",
    "bmode_fvec = jax.jit(jax.jacrev(fvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "m = 1000\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "A = jax.random.normal(jax.random.PRNGKey(0), (m, n))\n",
    "b = jax.random.normal(jax.random.PRNGKey(0), (m, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output had shape: (1000,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m J \u001b[38;5;241m=\u001b[39m jac_fvec(x, A, b)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(J\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgrad_fvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 18 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn/lib/python3.10/site-packages/jax/_src/api.py:1103\u001b[0m, in \u001b[0;36m_check_scalar\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(aval, ShapedArray):\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m aval\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m ():\n\u001b[0;32m-> 1103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad abstract value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output had shape: (1000,)."
     ]
    }
   ],
   "source": [
    "J = jac_fvec(x, A, b)\n",
    "print(J.shape)\n",
    "grad_fvec(x, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 7.94101434110216e-16\n",
      "Check shape (1000, 1000) (1000, 1000)\n",
      "Time forward mode\n",
      "16 ms ± 318 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time backward mode\n",
      "15.7 ms ± 309 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check correctness\", jnp.linalg.norm(fmode_fvec(x, A, b) - bmode_fvec(x, A, b)))\n",
    "print(\"Check shape\", fmode_fvec(x, A, b).shape, bmode_fvec(x, A, b).shape)\n",
    "print(\"Time forward mode\")\n",
    "%timeit fmode_fvec(x, A, b).block_until_ready()\n",
    "print(\"Time backward mode\")\n",
    "%timeit bmode_fvec(x, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 1000\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "A = jax.random.normal(jax.random.PRNGKey(0), (m, n))\n",
    "b = jax.random.normal(jax.random.PRNGKey(0), (m, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 8.043682175330496e-16\n",
      "Check shape (1000, 10) (1000, 10)\n",
      "Time forward mode\n",
      "44.1 µs ± 2.06 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Time backward mode\n",
      "2.7 ms ± 22.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check correctness\", jnp.linalg.norm(fmode_fvec(x, A, b) - bmode_fvec(x, A, b)))\n",
    "print(\"Check shape\", fmode_fvec(x, A, b).shape, bmode_fvec(x, A, b).shape)\n",
    "print(\"Time forward mode\")\n",
    "%timeit fmode_fvec(x, A, b).block_until_ready()\n",
    "print(\"Time backward mode\")\n",
    "%timeit bmode_fvec(x, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hessian-by-vector product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hvp(f, x, z, *args):\n",
    "    def g(x):\n",
    "        return f(x, *args)\n",
    "    return jax.jvp(jax.grad(g), (x,), (z,))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 3000\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "A = jax.random.normal(jax.random.PRNGKey(0), (n, n))\n",
    "b = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "z = jax.random.normal(jax.random.PRNGKey(0), (n, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check correctness 8.590867785772636e-10\n",
      "Time for hvp by hands\n",
      "17.9 ms ± 174 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for hvp via jvp, NO jit\n",
      "13.2 ms ± 70.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for hvp via jvp, WITH jit\n",
      "6.38 ms ± 32.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check correctness\", jnp.linalg.norm(2 * A.T @ (A @ z) - hvp(f, x, z, A, b)))\n",
    "print(\"Time for hvp by hands\")\n",
    "%timeit (2 * A.T @ (A @ z)).block_until_ready()\n",
    "print(\"Time for hvp via jvp, NO jit\")\n",
    "%timeit hvp(f, x, z, A, b).block_until_ready()\n",
    "print(\"Time for hvp via jvp, WITH jit\")\n",
    "%timeit jax.jit(hvp, static_argnums=0)(f, x, z, A, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- JAX is a simple and extensible tool in the problem where autodiff is crucial\n",
    "- JIT is a key to fast Python code\n",
    "- Input/output dimensions are important\n",
    "- Hessian matvec is faster than explicit hessian matrix by vector product"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
